### This `config_sources` block with `include:` is required to reference other files
config_sources:
  include:

extensions:
  health_check:
    endpoint: 0.0.0.0:13133
  http_forwarder:
    ingress:
      endpoint: 0.0.0.0:6060
    egress:
      endpoint: "${SPLUNK_API_URL}"
      # Use instead when sending to gateway
      #endpoint: "${SPLUNK_GATEWAY_URL}"
  zpages:
  memory_ballast:
    size_mib: ${SPLUNK_BALLAST_SIZE_MIB}

receivers:
  hostmetrics:
    collection_interval: 10s
    scrapers:
      cpu:
      disk:
      filesystem:
      memory:
      network:
      load:
      paging:
      processes:
  jaeger:
    protocols:
      grpc:
        endpoint: 0.0.0.0:14250
      thrift_binary:
        endpoint: 0.0.0.0:6832
      thrift_compact:
        endpoint: 0.0.0.0:6831
      thrift_http:
        endpoint: 0.0.0.0:14268
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
  # This section is used to collect the OpenTelemetry Collector metrics
  # Even if just a Splunk APM customer, these metrics are included
  prometheus/internal:
    config:
      scrape_configs:
      - job_name: 'otel-collector'
        scrape_interval: 10s
        static_configs:
        - targets: ['0.0.0.0:8888']
        metric_relabel_configs:
          - source_labels: [ __name__ ]
            regex: '.*grpc_io.*'
            action: drop
  ### This is where the fun begins.
  ### Your Snowflake receiver uses the `snowflake` receiver
  snowflake:
    username: ${SNOWFLAKE_USER}
    password: ${SNOWFLAKE_PASS}
    ### `account` is of the `account_identifier` from the login screen url for snowflake
    ### this format `<account_identifier>.snowflakecomputing.com`
    ### Azure Example: NP06963.canada-central.azure 
    ### AWS Example: xnb00000.us-east-1
    account: xnb00000.us-east-1 
    # Default is `COMPUTE_WH`
    warehouse: COMPUTE_WH
    collection_interval: 10m
    ### Enable non-default metrics
    metrics:
      snowflake.billing.cloud_service.total:
        enabled: true
      snowflake.billing.total_credit.total:
        enabled: true
      snowflake.billing.virtual_warehouse.total:
        enabled: true
      snowflake.billing.warehouse.cloud_service.total:
        enabled: true
      snowflake.billing.warehouse.total_credit.total:
        enabled: true
      snowflake.billing.warehouse.virtual_warehouse.total:
        enabled: true
      snowflake.logins.total:
        enabled: true 
      snowflake.query.bytes_spilled.local.avg:
        enabled: true 
      snowflake.query.bytes_spilled.remote.avg:
        enabled: true
      snowflake.query.data_scanned_cache.avg:
        enabled: true 
      snowflake.query.partitions_scanned.avg:
        enabled: true
      snowflake.rows_inserted.avg:
        enabled: true
      snowflake.rows_deleted.avg:
        enabled: true
      snowflake.rows_produced.avg:
        enabled: true
      snowflake.rows_unloaded.avg:
        enabled: true
      snowflake.rows_updated.avg:
        enabled: true
      snowflake.session_id.count:
        enabled: true
      snowflake.pipe.credits_used.total:
        enabled: true
      snowflake.storage.failsafe_bytes.total:
        enabled: true 
  signalfx:
    endpoint: 0.0.0.0:9943
  zipkin:
    endpoint: 0.0.0.0:9411

processors:
  batch:
  # Enabling the memory_limiter is strongly recommended for every pipeline.
  # Configuration is based on the amount of memory allocated to the collector.
  # For more information about memory limiter, see
  # https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/memorylimiter/README.md
  memory_limiter:
    check_interval: 2s
    limit_mib: ${SPLUNK_MEMORY_LIMIT_MIB}
  # detect if the collector is running on a cloud system
  # important for creating unique cloud provider dimensions
  resourcedetection:
    detectors: [system, gce, ecs, ec2, azure]
    override: false

  # Same as above but overrides resource attributes set by receivers
  resourcedetection/internal:
    detectors: [system, gce, ecs, ec2, azure]
    override: true    

exporters:
  # Traces
  sapm:
    access_token: "${SPLUNK_ACCESS_TOKEN}"
    endpoint: "${SPLUNK_TRACE_URL}"
  # Metrics + Events
  signalfx:
    access_token: "${SPLUNK_ACCESS_TOKEN}"
    api_url: "${SPLUNK_API_URL}"
    ingest_url: "${SPLUNK_INGEST_URL}"
    sync_host_metadata: true
    correlation:
  # Logs
  splunk_hec:
    token: "${SPLUNK_HEC_TOKEN}"
    endpoint: "${SPLUNK_HEC_URL}"
    source: "otel"
    sourcetype: "otel"
  # Send to gateway
  otlp:
    endpoint: "${SPLUNK_GATEWAY_URL}:4317"
    insecure: true
  # Debug
  logging:
    loglevel: info

service:
  extensions: [health_check, http_forwarder, zpages, memory_ballast]
  pipelines:
    traces:
      receivers: [jaeger, otlp, zipkin]
      processors:
      - memory_limiter
      - batch
      - resourcedetection
      exporters: [sapm, signalfx, splunk_hec]
      # Use instead when sending to gateway
      #exporters: [otlp, signalfx]
    metrics:
      ### This pipline uses our `snowflake` receiver and sends our Snowflake metrics
      receivers: [snowflake, hostmetrics, otlp, signalfx]
      processors: [memory_limiter, batch, resourcedetection]
      exporters: [signalfx]
      # Use instead when sending to gateway
      #exporters: [otlp]
    metrics/internal:
      receivers: [prometheus/internal]
      processors: [memory_limiter, batch, resourcedetection/internal]
      exporters: [signalfx]
      # Use instead when sending to gateway
      #exporters: [otlp]
    logs/signalfx:
      receivers: [signalfx]
      processors: [memory_limiter, batch]
      exporters: [splunk_hec]
      # Use instead when sending to gateway
      #exporters: [otlp]
